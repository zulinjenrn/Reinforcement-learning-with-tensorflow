{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "----oT选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "----oT选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "----oT选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "----oT选择的A值 right\n",
      "q_predict 0.0\n",
      "o----T                          选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.09000000000000001\n",
      "----oT选择的A值 right\n",
      "q_predict 0.1\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "o----T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.008100000000000001\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.009000000000000001\n",
      "q_target 0.171\n",
      "----oT选择的A值 right\n",
      "q_predict 0.19\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 0.0007290000000000002\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0008100000000000002\n",
      "q_target 0.022680000000000002\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.025200000000000004\n",
      "q_target 0.24390000000000003\n",
      "----oT选择的A值 right\n",
      "q_predict 0.271\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.0\n",
      "q_target 6.561000000000002e-05\n",
      "-o---T选择的A值 right\n",
      "q_predict 7.290000000000002e-05\n",
      "q_target 0.0026973000000000006\n",
      "--o--T选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.0003018060000000001\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0003353400000000001\n",
      "q_target 0.0026973000000000006\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.0029970000000000005\n",
      "q_target 0.042363000000000005\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.04707000000000001\n",
      "q_target 0.30951000000000006\n",
      "----oT选择的A值 right\n",
      "q_predict 0.34390000000000004\n",
      "o----T                          选择的A值 right\n",
      "q_predict 6.561000000000002e-06\n",
      "q_target 0.0005143824000000002\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0005715360000000002\n",
      "q_target 0.006240240000000002\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.006933600000000002\n",
      "q_target 0.06598260000000002\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.07331400000000002\n",
      "q_target 0.368559\n",
      "----oT选择的A值 right\n",
      "q_predict 0.40951000000000004\n",
      "o----T                          选择的A值 right\n",
      "q_predict 5.7343140000000026e-05\n",
      "q_target 0.0010245657600000003\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0011384064000000004\n",
      "q_target 0.011554650000000003\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.012838500000000003\n",
      "q_target 0.09255465000000002\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.10283850000000001\n",
      "q_target 0.42170310000000005\n",
      "----oT选择的A值 right\n",
      "q_predict 0.46855900000000006\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.00015406540200000004\n",
      "q_target 0.0019620276840000006\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.0021800307600000008\n",
      "q_target 0.018729103500000004\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.020810115000000004\n",
      "q_target 0.121252464\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.13472496\n",
      "q_target 0.4695327900000001\n",
      "----oT选择的A值 right\n",
      "q_predict 0.5217031000000001\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.00033486163020000014\n",
      "q_target 0.003451444230600001\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.003834938034000001\n",
      "q_target 0.027768914910000005\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.030854349900000005\n",
      "q_target 0.15138516870000002\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.16820574300000002\n",
      "q_target 0.5125795110000001\n",
      "----oT选择的A值 right\n",
      "q_predict 0.5695327900000001\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.0006465198902400003\n",
      "q_target 0.005605502149440002\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.006228335721600002\n",
      "q_target 0.03861668860200001\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.042907431780000005\n",
      "q_target 0.18237880782000002\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.20264311980000002\n",
      "q_target 0.5513215599000001\n",
      "----oT选择的A值 right\n",
      "q_predict 0.6125795110000001\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.0011424181161600005\n",
      "q_target 0.008520453908676002\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.009467171009640002\n",
      "q_target 0.051169112445600004\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.056854569384000006\n",
      "q_target 0.21375986742900002\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.23751096381000003\n",
      "q_target 0.5861894039100001\n",
      "----oT选择的A值 right\n",
      "q_predict 0.6513215599000001\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.0018802216954116008\n",
      "q_target 0.012273628637912402\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.013637365153236002\n",
      "q_target 0.06529058926965\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.07254509918850001\n",
      "q_target 0.24514092703800003\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.27237880782\n",
      "q_target 0.617570463519\n",
      "----oT选择的A值 left\n",
      "q_predict 0.0\n",
      "q_target 0.27620817605091\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.3068979733899\n",
      "q_target 0.617570463519\n",
      "----oT选择的A值 right\n",
      "q_predict 0.68618940391\n",
      "o----T                          选择的A值 right\n",
      "q_predict 0.002919562389661681\n",
      "q_target 0.016922418808389662\n",
      "-o---T选择的A值 right\n",
      "q_predict 0.018802687564877404\n",
      "q_target 0.08082421377610502\n",
      "--o--T选择的A值 right\n",
      "q_predict 0.08980468197345001\n",
      "q_target 0.304168700162529\n",
      "---o-T选择的A值 right\n",
      "q_predict 0.33796522240281\n",
      "q_target 0.6458134171671001\n",
      "----oT选择的A值 right\n",
      "q_predict 0.717570463519\n",
      "                                \n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# %load treasure_on_right.py\n",
    "\"\"\"\n",
    "A simple example for Reinforcement Learning using table lookup Q-learning method.\n",
    "An agent \"o\" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\n",
    "Run this program and to see how the agent will improve its strategy of finding the treasure.\n",
    "\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # reproducible\n",
    "\n",
    "\n",
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "EPSILON = 0.9   # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 13   # maximum episodes\n",
    "FRESH_TIME = 0.3    # fresh time for one move\n",
    "\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    # print(table)    # show table\n",
    "    return table\n",
    "\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(ACTIONS)#######获得随机概率值\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "    return action_name\n",
    "\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            print(\"选择的A值\",A)\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            print(\"q_predict\",q_predict)\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "                print(\"q_target\",q_target)\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    print(\"打印qTable表格如下↓👇↓\" ) \n",
    "    print(table)# show table\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印qTable表格如下↓👇↓\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left  right\n",
       "0   0.0    0.0\n",
       "1   0.0    0.0\n",
       "2   0.0    0.0\n",
       "3   0.0    0.0\n",
       "4   0.0    0.0\n",
       "5   0.0    0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_q_table(N_STATES, ACTIONS)  ###测试def定义的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :] ###iloc是行列号索引所有，左边行，右边列,索引 第state行的所有列\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "    return action_name                      #返回最大值的索引号，索引号是index值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印qTable表格如下↓👇↓\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--------------------\n",
      "--------------------\n",
      "left     True\n",
      "right    True\n",
      "Name: 4, dtype: bool\n",
      "--------------------\n",
      "left\n"
     ]
    }
   ],
   "source": [
    "q_table = build_q_table(N_STATES, ACTIONS)\n",
    "print(\"--------------------\")\n",
    "state_actions = q_table.iloc[4, :]\n",
    "print('--------------------') \n",
    "print(state_actions == 0)\n",
    "print('--------------------')\n",
    "((state_actions == 0).all()) #判断state_actions里面的所有元素是否都为零，是则返回真，不是则返回假元素是否为空\n",
    "action_name = state_actions.idxmax() #index值是left和right\n",
    "print(action_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate 撞墙时的状态是6-2=4，所以等到状态为4时，再向右边走就到达终点，奖励加1\n",
    "            S_ = 'terminal'     #此处注意S和S_的区别，不一样的\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy.random import rand\n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "# 使用seed\n",
    "a = np.random.uniform(5)\n",
    "print('第一次列表a：',a)\n",
    "np.random.seed(2)\n",
    "a = np.random.uniform(5)\n",
    "print('第二次列表a：',a)\n",
    "\n",
    "#使用np.random.seed()的目的是生产相同的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
